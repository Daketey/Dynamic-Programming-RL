{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import keras\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(64)\n",
      "Action space: Discrete(4)\n",
      "observation space: Discrete(64)\n",
      "action space: Discrete(4)\n",
      "taking action\n",
      "new observation code: 0\n",
      "reward: 0.0\n",
      "is game over?: False\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "env.reset()\n",
    "\n",
    "#plt.imshow(env.render('rgb_array'))\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "print(\"taking action\")\n",
    "new_obs, reward, is_done, _ = env.step(1)\n",
    "\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States:  64\n",
      "Number of Actions:  4\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "print(\"Number of States: \", n_states)\n",
    "print(\"Number of Actions: \", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Policy Matrix (64, 4)\n"
     ]
    }
   ],
   "source": [
    "#Input policy to be evaluated\n",
    "\n",
    "policy = np.ones((n_states , n_actions))/n_actions\n",
    "print(\"Shape of Policy Matrix\" , policy.shape)\n",
    "\n",
    "#Initialize array V(s) = 0\n",
    "\n",
    "V = np.zeros(n_states)\n",
    "discount_factor = 1.0\n",
    "theta = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 4, 0.0, False),\n",
       " (0.3333333333333333, 3, 0.0, False),\n",
       " (0.3333333333333333, 2, 0.0, False)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[3][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor = 1.0 , theta = 0.00001 ):\n",
    "    #Repeat:\n",
    "    while True:\n",
    "        delta = 0.0        #delta <- 0\n",
    "    \n",
    "        for s in range(n_states):   #For each state:\n",
    "            v = 0\n",
    "            c = 0\n",
    "            for a , policy_prob in enumerate(policy[s]):                            #Expression---\n",
    "                for prob, next_state, reward, done  in env.P[s][a]:\n",
    "                    v += policy_prob * prob *(reward + discount_factor*V[next_state])     # V(s) = sum(policy(a|s) * sum(prob(s', r|s,a)) * (reward + gamma*V(s'))                                                       #---\n",
    "            delta = max(delta , abs(v - V[s]))            #delta <- max(delta , |v - V[s]|)\n",
    "            V[s] = v\n",
    "\n",
    "        if(delta<theta):                    #terminate on delta < theta (theta is a small positive number)\n",
    "            break\n",
    "    return V\n",
    "        # Output V ~= vpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpi = policy_evaluation(policy , env) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.77291852e-03, 2.05231760e-03, 2.71200668e-03, 4.02682148e-03,\n",
       "       6.45060006e-03, 9.69730771e-03, 1.33323211e-02, 1.58487758e-02,\n",
       "       1.52729353e-03, 1.69427642e-03, 2.07670686e-03, 2.93669579e-03,\n",
       "       5.64863740e-03, 9.33260477e-03, 1.44768170e-02, 1.83922528e-02,\n",
       "       1.13470573e-03, 1.13113238e-03, 9.71534634e-04, 0.00000000e+00,\n",
       "       3.88354185e-03, 7.51853274e-03, 1.68629320e-02, 2.48647709e-02,\n",
       "       7.60085659e-04, 7.30250376e-04, 6.80298040e-04, 7.62713639e-04,\n",
       "       2.37149567e-03, 0.00000000e+00, 2.05998575e-02, 3.93483800e-02,\n",
       "       4.24537163e-04, 3.53058351e-04, 2.58339098e-04, 0.00000000e+00,\n",
       "       4.84014067e-03, 1.15878559e-02, 2.61929417e-02, 7.25859868e-02,\n",
       "       1.65174830e-04, 0.00000000e+00, 0.00000000e+00, 1.44772835e-03,\n",
       "       5.40187309e-03, 1.53198027e-02, 0.00000000e+00, 1.52219331e-01,\n",
       "       7.23056825e-05, 0.00000000e+00, 1.09152955e-04, 3.89220327e-04,\n",
       "       0.00000000e+00, 4.42895328e-02, 0.00000000e+00, 3.84073024e-01,\n",
       "       5.24361145e-05, 3.32439149e-05, 4.74511163e-05, 0.00000000e+00,\n",
       "       5.39461066e-02, 1.61838352e-01, 3.87279448e-01, 0.00000000e+00])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
