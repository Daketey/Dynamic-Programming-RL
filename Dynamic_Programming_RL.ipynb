{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import keras\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(64)\n",
      "Action space: Discrete(4)\n",
      "observation space: Discrete(64)\n",
      "action space: Discrete(4)\n",
      "taking action\n",
      "new observation code: 1\n",
      "reward: 0.0\n",
      "is game over?: False\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "env.reset()\n",
    "\n",
    "#plt.imshow(env.render('rgb_array'))\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "print(\"taking action\")\n",
    "new_obs, reward, is_done, _ = env.step(1)\n",
    "\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States:  64\n",
      "Number of Actions:  4\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "print(\"Number of States: \", n_states)\n",
    "print(\"Number of Actions: \", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Policy Matrix (64, 4)\n"
     ]
    }
   ],
   "source": [
    "#Input policy to be evaluated\n",
    "\n",
    "policy = np.ones((n_states , n_actions))/n_actions\n",
    "print(\"Shape of Policy Matrix\" , policy.shape)\n",
    "\n",
    "#Initialize array V(s) = 0\n",
    "\n",
    "V = np.zeros(n_states)\n",
    "discount_factor = 1.0\n",
    "theta = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 4, 0.0, False),\n",
       " (0.3333333333333333, 3, 0.0, False),\n",
       " (0.3333333333333333, 2, 0.0, False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function which calculates the value for all actions\n",
    "def one_step_lookahead(state , V , discount_factor = 1.0):\n",
    "    v = np.zeros(n_actions)\n",
    "    for a in range(n_actions):\n",
    "        for prob, next_state, reward, done  in env.P[state][a]:\n",
    "            v[a] = prob*(reward+discount_factor*V[next_state])\n",
    "    return v\n",
    "\"\"\"\"\n",
    "Formula:\n",
    "      V = sum(prob(s', r|s,a)) * (reward + gamma*V(s')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor = 1.0 , theta = 0.00001 ):\n",
    "    #Repeat:\n",
    "    while True:\n",
    "        delta = 0.0        #delta <- 0\n",
    "    \n",
    "        for s in range(n_states):   #For each state:\n",
    "            v = 0\n",
    "            for a , policy_prob in enumerate(policy[s]):                            #Expression---\n",
    "                for prob, next_state, reward, done  in env.P[s][a]:\n",
    "                    v += policy_prob * prob *(reward + discount_factor*V[next_state])     # V(s) = sum(policy(a|s) * sum(prob(s', r|s,a)) * (reward + gamma*V(s')) \n",
    "            delta = max(delta , abs(v - V[s]))            #delta <- max(delta , |v - V[s]|)\n",
    "            V[s] = v\n",
    "\n",
    "        if(delta<theta):                    #terminate on delta < theta (theta is a small positive number)\n",
    "            break\n",
    "    return V\n",
    "        # Output V ~= vpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value:  [2.84835099e-03 3.14099803e-03 3.83307468e-03 5.31436453e-03\n",
      " 8.61087838e-03 1.30662629e-02 1.81048038e-02 2.11866439e-02\n",
      " 2.53651262e-03 2.73445587e-03 3.03991188e-03 3.49704871e-03\n",
      " 7.45066365e-03 1.24821542e-02 2.00606634e-02 2.42676012e-02\n",
      " 2.01868693e-03 2.01804649e-03 1.78767009e-03 0.00000000e+00\n",
      " 6.37212215e-03 9.93466621e-03 2.34317315e-02 3.15552911e-02\n",
      " 1.49608503e-03 1.52925294e-03 2.32221776e-03 4.66264019e-03\n",
      " 1.16654746e-02 0.00000000e+00 2.88049879e-02 4.69664404e-02\n",
      " 9.36955988e-04 7.67873022e-04 7.74072587e-04 0.00000000e+00\n",
      " 2.39616613e-02 3.78061768e-02 3.94484063e-02 8.05389966e-02\n",
      " 5.45444953e-04 0.00000000e+00 0.00000000e+00 1.27795527e-02\n",
      " 3.40788072e-02 8.94568690e-02 0.00000000e+00 1.55202126e-01\n",
      " 6.98915628e-04 0.00000000e+00 1.70394036e-03 4.25985091e-03\n",
      " 0.00000000e+00 1.96485623e-01 0.00000000e+00 3.85067375e-01\n",
      " 8.51930150e-04 8.51948220e-04 8.51970181e-04 0.00000000e+00\n",
      " 2.50000000e-01 5.00000000e-01 7.50000000e-01 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "vpi = policy_evaluation(policy , env) \n",
    "print(\"Value: \" , vpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env , discount_factor=1.0):\n",
    "    #1. Initialization\n",
    "    policy = np.ones((n_states , n_actions))/n_actions\n",
    "\n",
    "    while True:\n",
    "        #2. Policy Evaluation\n",
    "        V = policy_evaluation(policy , env) \n",
    "    \n",
    "        #3. Policy Improvement\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in range(n_states):  #for each s in States\n",
    "            old_action = np.argmax(policy[s])            #old_action <- policy[s]\n",
    "            best_action = np.argmax(one_step_lookahead(s , V))  #policy[s]<- sum(prob(s', r|s,a)) * (reward + gamma*V(s')) \n",
    "            if old_action != best_action:           #if old-action != policy(s), then policy-stable -> false\n",
    "                policy_stable = False \n",
    "            policy[s] = np.eye(n_actions)[best_action]   #(Making the index of policy for the best action as 1)\n",
    "        if policy_stable:         #if policy-stable, then stop and return V  and policy   \n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value:  [2.85609659e-03 3.14647457e-03 3.83650250e-03 5.31635376e-03\n",
      " 8.61208165e-03 1.30670364e-02 1.81053808e-02 2.11871685e-02\n",
      " 2.54248182e-03 2.73814059e-03 3.04187225e-03 3.49793726e-03\n",
      " 7.45123425e-03 1.24825294e-02 2.00609799e-02 2.42679594e-02\n",
      " 2.02304626e-03 2.02049733e-03 1.78872371e-03 0.00000000e+00\n",
      " 6.37241284e-03 9.93481902e-03 2.34318765e-02 3.15554964e-02\n",
      " 1.49905085e-03 1.53095990e-03 2.32309838e-03 4.66303422e-03\n",
      " 1.16657028e-02 0.00000000e+00 2.88050319e-02 4.69665405e-02\n",
      " 9.38608249e-04 7.68575399e-04 7.74366128e-04 0.00000000e+00\n",
      " 2.39616613e-02 3.78061768e-02 3.94484233e-02 8.05390422e-02\n",
      " 5.46207537e-04 0.00000000e+00 0.00000000e+00 1.27795527e-02\n",
      " 3.40788072e-02 8.94568690e-02 0.00000000e+00 1.55202143e-01\n",
      " 6.99378871e-04 0.00000000e+00 1.70394036e-03 4.25985091e-03\n",
      " 0.00000000e+00 1.96485623e-01 0.00000000e+00 3.85067381e-01\n",
      " 8.51923059e-04 8.51944331e-04 8.51970181e-04 0.00000000e+00\n",
      " 2.50000000e-01 5.00000000e-01 7.50000000e-01 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "policy , V = policy_iteration(env)\n",
    "print(\"Policy: \", policy)\n",
    "print(\"Value: \" , V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env , discount_factor = 1.0 , theta = 0.00001 ):\n",
    "    V = np.zeros(n_states)     #Initialize array V arbitrarily\n",
    "    \n",
    "    #Repeat\n",
    "    while True:    \n",
    "        delta = 0.0        #delta <- 0\n",
    "        for s in range(n_states):   #For each s in state:\n",
    "            v = V[s]               # v <- V(s)\n",
    "            V[s] = max(one_step_lookahead(s, V , discount_factor = discount_factor))  # V(s) = max(sum(prob(s', r|s,a)) * (reward + gamma*V(s'))) \n",
    "            delta = max(delta , abs(v - V[s]))            #delta <- max(delta , |v - V[s]|)\n",
    "            \n",
    "        if(delta<theta):                    #terminate on delta < theta (theta is a small positive number)\n",
    "            break\n",
    "\n",
    "    #Output a deterministic policy pi such that:\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for s in range(n_states):\n",
    "        best_action = np.argmax(one_step_lookahead(s , V))\n",
    "        policy[s] = np.eye(n_actions)[best_action]\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value:  [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.64502927e-06\n",
      " 1.69350878e-05 5.08052634e-05 1.52415790e-04 4.57247371e-04\n",
      " 0.00000000e+00 0.00000000e+00 5.64502927e-06 1.69350878e-05\n",
      " 5.08052634e-05 1.52415790e-04 4.57247371e-04 1.37174211e-03\n",
      " 0.00000000e+00 5.64502927e-06 1.69350878e-05 0.00000000e+00\n",
      " 1.52415790e-04 4.57247371e-04 1.37174211e-03 4.11522634e-03\n",
      " 5.64502927e-06 1.69350878e-05 5.08052634e-05 1.52415790e-04\n",
      " 4.57247371e-04 0.00000000e+00 4.11522634e-03 1.23456790e-02\n",
      " 1.88167642e-06 5.64502927e-06 1.69350878e-05 0.00000000e+00\n",
      " 1.37174211e-03 4.11522634e-03 1.23456790e-02 3.70370370e-02\n",
      " 6.27225474e-07 0.00000000e+00 0.00000000e+00 1.37174211e-03\n",
      " 4.11522634e-03 1.23456790e-02 0.00000000e+00 1.11111111e-01\n",
      " 1.88167642e-06 0.00000000e+00 1.52415790e-04 4.57247371e-04\n",
      " 0.00000000e+00 3.70370370e-02 0.00000000e+00 3.33333333e-01\n",
      " 5.64502927e-06 1.69350878e-05 5.08052634e-05 0.00000000e+00\n",
      " 3.70370370e-02 1.11111111e-01 3.33333333e-01 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "policy , V = value_iteration(env)\n",
    "print(\"Policy: \", policy)\n",
    "print(\"Value: \" , V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value:  [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.64502927e-06\n",
      " 1.69350878e-05 5.08052634e-05 1.52415790e-04 4.57247371e-04\n",
      " 0.00000000e+00 0.00000000e+00 5.64502927e-06 1.69350878e-05\n",
      " 5.08052634e-05 1.52415790e-04 4.57247371e-04 1.37174211e-03\n",
      " 0.00000000e+00 5.64502927e-06 1.69350878e-05 0.00000000e+00\n",
      " 1.52415790e-04 4.57247371e-04 1.37174211e-03 4.11522634e-03\n",
      " 5.64502927e-06 1.69350878e-05 5.08052634e-05 1.52415790e-04\n",
      " 4.57247371e-04 0.00000000e+00 4.11522634e-03 1.23456790e-02\n",
      " 1.88167642e-06 5.64502927e-06 1.69350878e-05 0.00000000e+00\n",
      " 1.37174211e-03 4.11522634e-03 1.23456790e-02 3.70370370e-02\n",
      " 6.27225474e-07 0.00000000e+00 0.00000000e+00 1.37174211e-03\n",
      " 4.11522634e-03 1.23456790e-02 0.00000000e+00 1.11111111e-01\n",
      " 1.88167642e-06 0.00000000e+00 1.52415790e-04 4.57247371e-04\n",
      " 0.00000000e+00 3.70370370e-02 0.00000000e+00 3.33333333e-01\n",
      " 5.64502927e-06 1.69350878e-05 5.08052634e-05 0.00000000e+00\n",
      " 3.70370370e-02 1.11111111e-01 3.33333333e-01 0.00000000e+00]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
